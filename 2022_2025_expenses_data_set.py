# -*- coding: utf-8 -*-
"""2022 - 2025 expenses data set

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJWJkOhYNWrru-reWWjVyfWklhKQBGOU

Installing Required Packages
"""

pip install pandas scikit-learn nltk  > /dev/null 2>&1

pip install matplotlib scikit-learn numpy  > /dev/null 2>&1

pip install kneed  > /dev/null 2>&1

"""Importing Libraries"""

import pandas as pd
import numpy as np
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import LabelEncoder
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
from kneed import KneeLocator

"""Upload Dataset - Make sure it's a CSV File"""

from google.colab import files
print("Upload your file...")
uploaded = files.upload()
file_name = next(iter(uploaded))
df = pd.read_csv(file_name, encoding="ISO-8859-1")

"""Data preprocessing and cleaning

Downloading "Stopword" remover
"""

nltk.download('stopwords')
from nltk.corpus import stopwords

"""Clean the material description in "Text"
"""

# Custom stopword list (add domain-specific irrelevant words)
custom_stopwords = set(stopwords.words('english')).union({
    'pcs', 'piece', 'spare', 'component', 'material', 'set'
})

# Clean free-text descriptions only
def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'[^a-z0-9\s\-\/]', '', text)  # Keep part number friendly characters
    tokens = text.split()
    tokens = [t for t in tokens if t not in custom_stopwords]
    return ' '.join(tokens)

df['CleanText'] = df['Text'].apply(clean_text)
df['CleanDescription'] = df['Material Description'].apply(clean_text)

"""# Data preprocessing
Remove special words
"""

# Convert date and amount to the correct data types
df['Document Date'] = pd.to_datetime(df['Document Date'])
df['Amount in local currency'] = pd.to_numeric(df['Amount in local currency'], errors='coerce')
df["YearMonth"] = df["Document Date"].dt.to_period("M")

# üîç Remove negative transactions (represent payments or credits)
df = df[df['Amount in local currency'] > 0]

# üîç Ask for keywords to remove
keywords_input = input("Enter keywords to filter out (separated by commas). Example - shipping, expedite, fee, service, freight, repair, overnight, next day, after hours, delivery : ")

# üîÑ Normalize input: Convert to lowercase and strip spaces around each word
keywords = [kw.strip().lower() for kw in keywords_input.split(',')]

# Convert the "CleanText" column to lowercase for easy searching
df['CleanText'] = df['CleanText'].str.lower()

# üîç Build a regex pattern to ignore special characters
# This pattern removes any non-alphanumeric characters for matching
pattern = '|'.join([r'\b' + re.escape(keyword) + r'\b' for keyword in keywords])

# üîÑ Remove rows that match the pattern
initial_row_count = len(df)
df = df[~df['CleanText'].str.contains(pattern, na=False, regex=True)]
rows_removed = initial_row_count - len(df)

"""Remove columns with no information (no material number, no text, no description)"""

# Step 1: Identify entries that match all three criteria:
# 1. No text in "Text" column (NaN or empty)
# 2. No description in "Description" column (NaN or empty)
# 3. No material number or the word "multiple" in the "Material" column
conditions_to_remove = (
    (df['Text'].isna() | (df['Text'].str.strip() == "")) &  # Empty or NaN "Text"
    (df['Material Description'].isna() | (df['Material Description'].str.strip() == "")) &  # Empty or NaN "Description"
    (df['Material'].isna() | (df['Material'].str.lower() == "multiple"))  # NaN or "multiple" in "Material"
)

# Step 2: Count the rows before removal for tracking
initial_row_count = len(df)

# Step 3: Remove the rows that meet all three conditions
df = df[~conditions_to_remove]

"""Vectorization of tex descrition

The TF-IDF vectorizer will:

  1. Break each description into individual words (called "tokens")

2. Count how often each word appears in each description (Term Frequency)

3. Reduce the weight of very common words that appear in many descriptions (Inverse Document Frequency)
"""

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['CleanText'])

"""Cluster with Density-Based Spatial Clustering of Applications with Noise (DBSCAN)

- Detects clusters of any shape

- Automatically finds "noise" points that don‚Äôt belong anywhere (great for dirty or inconsistent data!)

- Creates clusters as necessary
"""

clustering = DBSCAN(eps=0.47, min_samples=2, metric='cosine')
df['Cluster'] = clustering.fit_predict(X)

"""Create and assign new material number to missing material number entries"""

#Clusters = -1 are each considered unique materials
#Instead of assigning all -1 cluster items the same NEW-Mxxxxx code, this code assigns a unique ID for each based on its row index or a hash of the cleaned text.
def generate_unique_id(row):
    if row['Cluster'] == -1:
        return f"NEW-M#-{str(row.name).zfill(5)}"  # Or hash the text
    else:
        return f"NEW-M{str(row['Cluster']).zfill(5)}"

df['Material'] = df.apply(
    lambda row: row['Material'] if pd.notnull(row['Material']) and row['Material'] != ''
    else generate_unique_id(row),
    axis=1
)

def generate_new_id(cluster):
    return f"NEW-M{str(cluster).zfill(5)}"

df['Material'] = df.apply(
    lambda row: row['Material'] if pd.notnull(row['Material']) and row['Material'] != '' else generate_new_id(row['Cluster']),
    axis=1
)

"""# Defining Maximums, Minimums & Critical Parts"""

# Step 1: Input parameters
months_range = int(input("Enter number of months to calculate inventory range: "))
critical_price_threshold = float(input("Enter price in $ threshold for critical parts: $"))
critical_order_threshold = int(input("Enter order count threshold for critical parts in the month range you chose: "))

# Step 2: Flag single orders
df['order_count'] = df.groupby('Material')['Material'].transform('count')
df['Single_order_flag'] = df['order_count'] == 1

single_order_df = df[df['Single_order_flag']].copy()
df = df[~df['Single_order_flag']].copy()

# Step 3: Monthly order counts
monthly_orders = df.groupby(["Material", "YearMonth"]).size().reset_index(name="Orders")
monthly_orders["MonthStart"] = monthly_orders["YearMonth"].dt.to_timestamp()

results = []
for material, group in monthly_orders.groupby("Material"):
    group = group.sort_values("MonthStart")
    group.set_index("MonthStart", inplace=True)
    resampled = group["Orders"].resample("MS").sum()
    rolling_sum = resampled.rolling(window=months_range).sum().dropna()

    if rolling_sum.empty:
        max_inv = 1
        avg_inv = 1
    else:
        max_inv = int(rolling_sum.max())
        avg_inv = round(rolling_sum.mean())

    results.append({
        "Material": material,
        "Min Inventory": avg_inv,
        "Max Inventory": max_inv
    })

inventory_stats = pd.DataFrame(results)

# Step 4: Re-merge single order materials
single_order_flag = single_order_df[['Material', 'Single_order_flag']]
inventory_stats = inventory_stats.merge(single_order_flag, how='outer')
inventory_stats['Single_order_flag'] = inventory_stats['Single_order_flag'].notna()
inventory_stats.loc[inventory_stats['Single_order_flag'], 'Min Inventory'] = 1
inventory_stats.loc[inventory_stats['Single_order_flag'], 'Max Inventory'] = 1

# Merge back full DataFrame
df = pd.concat([df, single_order_df], ignore_index=True)
df["Year"] = df["Document Date"].dt.year


# Step 5: Mark Critical Parts based on Price AND Total Count Threshold

# üîç 1. Get all materials that meet the price threshold (at least one order with the price above the threshold)
high_cost = df[df["Amount in local currency"] >= critical_price_threshold]["Material"].unique()

# üîç 2. Get all materials that meet the count threshold across the entire dataset
order_counts = df.groupby("Material").size().reset_index(name="Total Orders")
high_demand = order_counts[order_counts["Total Orders"] >= critical_order_threshold]["Material"].unique()

# üîç 3. Find materials that meet **both conditions** (price and count)
critical_parts = set(high_cost).intersection(set(high_demand))

# üîç 4. Update the DataFrame to mark the critical parts
inventory_stats["Critical Part"] = inventory_stats["Material"].isin(critical_parts)
inventory_stats["Critical Part"] = inventory_stats["Critical Part"].map({True: "Critical", False: "Noncritical"})

"""Add description, cost & cost center"""

# Add 'Text' column to inventory_stats, filling missing values with 'N/A'
# Use .groupby and .first() to handle duplicate 'Material' values in df
material_text_mapping = df.groupby('Material')['CleanText'].first()
inventory_stats["Text"] = inventory_stats["Material"].map(material_text_mapping)
inventory_stats.fillna({"Text":"N/A"}, inplace=True)
# Add 'Amount in local currency' to inventory_stats (using the mean or any other aggregate for each material)
inventory_stats["Amount in local currency"] = inventory_stats["Material"].map(df.groupby("Material")["Amount in local currency"].mean())
# Add 'Material Description' column to inventory_stats, filling missing values with 'N/A'
# Use .groupby and .first() to handle duplicate 'Material' values in df
material_description_mapping = df.groupby('Material')['CleanDescription'].first()
inventory_stats["Material Description"] = inventory_stats["Material"].map(material_description_mapping)
inventory_stats.fillna({"Text":"N/A"}, inplace=True)

#Add "cost center" column to invetory_stats
inventory_stats["Cost Center Name"] = inventory_stats["Material"].map(df.groupby("Material")["Cost center name"].first())

# Save the result as a CSV file
inventory_stats.to_csv("expenses_stats.csv", index=False)

# Automatically download the file
from google.colab import files
files.download('expenses_stats.csv')